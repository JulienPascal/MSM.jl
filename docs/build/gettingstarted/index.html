<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Getting started · MSM.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">MSM.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../installation/">Installation</a></li><li class="is-active"><a class="tocitem" href>Getting started</a><ul class="internal"><li><a class="tocitem" href="#Example-in-serial"><span>Example in serial</span></a></li><li><a class="tocitem" href="#Example-in-parallel"><span>Example in parallel</span></a></li></ul></li><li><a class="tocitem" href="../functions/">Functions and Types</a></li><li><a class="tocitem" href="../references/">References</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Getting started</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Getting started</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JulienPascal/MSM.jl/blob/master/docs/src/gettingstarted.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Getting-Started"><a class="docs-heading-anchor" href="#Getting-Started">Getting Started</a><a id="Getting-Started-1"></a><a class="docs-heading-anchor-permalink" href="#Getting-Started" title="Permalink"></a></h1><p>Our overarching goal is to find the parameter values <span>$\theta_{MSM}$</span> minimizing the following function:</p><p class="math-container">\[g(\theta; m*, W) = (m(\theta) - m*)&#39; W (m(\theta) - m*)\]</p><p>where <span>$m*$</span> is a vector of empirical moments, <span>$m(\theta)$</span> is a vector of moments calculated using simulated data, and <span>$W$</span> is carefully chosen weighting matrix. We also want to build confidence intervals for <span>$\theta_{MSM}$</span>.</p><p>While simple in theory (it is just a function minimization, right?), in practice many bad things can happen. The function <span>$g$</span> may fail in some areas of the parameter space; <span>$g$</span> may be stuck in some local minima; <span>$g$</span> is really slow and you do not have a strong prior regarding good starting values. <a href="https://github.com/JulienPascal/MSM.jl">MSM.jl</a> uses minimization algorithms that are robust to the problems mentioned above. You may choose between two options:</p><ol><li>Global minimization algorithms from <a href="https://github.com/robertfeldt/BlackBoxOptim.jl">BlackBoxOptim</a></li><li>A multistart algorithm using several local optimization routines from <a href="https://github.com/JuliaNLSolvers/Optim.jl">Optim.jl</a></li></ol><p>Let&#39;s follow a learning-by-doing approach. As a warm-up, let&#39;s first estimate parameters in serial. In a second step, we use several workers on a cluster.</p><h2 id="Example-in-serial"><a class="docs-heading-anchor" href="#Example-in-serial">Example in serial</a><a id="Example-in-serial-1"></a><a class="docs-heading-anchor-permalink" href="#Example-in-serial" title="Permalink"></a></h2><p>In a real-world scenario, you would probably use empirical data. Here, let&#39;s simulate a fake dataset.</p><pre><code class="language-julia">using MSM
using DataStructures
using OrderedCollections
using Random
using Distributions
using Statistics
using LinearAlgebra
using Plots
Random.seed!(1234)  #for replicability reasons
T = 100000          #number of periods
P = 2               #number of dependent variables
beta0 = rand(P)     #choose true coefficients by drawing from a uniform distribution on [0,1]
alpha0 = rand(1)[]  #intercept
theta0 = 0.0 #coefficient to create serial correlation in the error terms

# Generation of error terms
# row = individual dimension
# column = time dimension
U = zeros(T)
d = Normal()
U[1] = rand(d, 1)[] #first error term
for t = 2:T
    U[t] = rand(d, 1)[] + theta0*U[t-1]
end

# Let&#39;s simulate the dependent variables x_t
x = zeros(T, P)
d = Uniform(0, 5)
for p = 1:P
    x[:,p] = rand(d, T)
end

# Let&#39;s calculate the resulting y_t
y = zeros(T)
for t=1:T
    y[t] = alpha0 + x[t,1]*beta0[1] + x[t,2]*beta0[2] + U[t]
end

# Visualize data
p1 = scatter(x[1:100,1], y[1:100], xlabel = &quot;x1&quot;, ylabel = &quot;y&quot;, legend=:none, smooth=true)
p2 = scatter(x[1:100,2], y[1:100], xlabel = &quot;x2&quot;, ylabel = &quot;y&quot;, legend=:none, smooth=true)
p = plot(p1, p2);</code></pre><p><img src="../f-fake-data.svg" alt/></p><h3 id="Step-1:-Initializing-a-MSMProblem"><a class="docs-heading-anchor" href="#Step-1:-Initializing-a-MSMProblem">Step 1: Initializing a MSMProblem</a><a id="Step-1:-Initializing-a-MSMProblem-1"></a><a class="docs-heading-anchor-permalink" href="#Step-1:-Initializing-a-MSMProblem" title="Permalink"></a></h3><pre><code class="language-"># Select a global optimizer (see BlackBoxOptim.jl) and a local minimizer (see Optim.jl):
myProblem = MSMProblem(options = MSMOptions(maxFuncEvals=10000, globalOptimizer = :dxnes, localOptimizer = :LBFGS));</code></pre><h3 id="Step-2.-Set-empirical-moments-and-weight-matrix"><a class="docs-heading-anchor" href="#Step-2.-Set-empirical-moments-and-weight-matrix">Step 2. Set empirical moments and weight matrix</a><a id="Step-2.-Set-empirical-moments-and-weight-matrix-1"></a><a class="docs-heading-anchor-permalink" href="#Step-2.-Set-empirical-moments-and-weight-matrix" title="Permalink"></a></h3><p>Choose the set of empirical moments to match and the weight matrix <span>$W$</span> using the functions <code>set_empirical_moments!</code> and <code>set_weight_matrix!</code></p><pre><code class="language-">dictEmpiricalMoments = OrderedDict{String,Array{Float64,1}}()
dictEmpiricalMoments[&quot;mean&quot;] = [mean(y)] #informative on the intercept
dictEmpiricalMoments[&quot;mean_x1y&quot;] = [mean(x[:,1] .* y)] #informative on betas
dictEmpiricalMoments[&quot;mean_x2y&quot;] = [mean(x[:,2] .* y)] #informative on betas
dictEmpiricalMoments[&quot;mean_x1y^2&quot;] = [mean((x[:,1] .* y).^2)] #informative on betas
dictEmpiricalMoments[&quot;mean_x2y^2&quot;] = [mean((x[:,2] .* y).^2)] #informative on betas

W = Matrix(1.0 .* I(length(dictEmpiricalMoments)))#initialization
#Special case: diagonal matrix
#Sum of square percentage deviations from empirical moments
#(you may choose something else)
for (indexMoment, k) in enumerate(keys(dictEmpiricalMoments))
    W[indexMoment,indexMoment] = 1.0/(dictEmpiricalMoments[k][1])^2
end

set_empirical_moments!(myProblem, dictEmpiricalMoments)
set_weight_matrix!(myProblem, W)</code></pre><h3 id="Step-3.-Set-priors"><a class="docs-heading-anchor" href="#Step-3.-Set-priors">Step 3. Set priors</a><a id="Step-3.-Set-priors-1"></a><a class="docs-heading-anchor-permalink" href="#Step-3.-Set-priors" title="Permalink"></a></h3><p>Our &quot;prior&quot; belief regarding the parameter values is to be specified using <code>set_priors!()</code>. It is not fully a full-fledged prior probability distribution, but simply an initial guess for each parameter, as well as lower and upper bounds:</p><pre><code class="language-">dictPriors = OrderedDict{String,Array{Float64,1}}()
# Of the form: [initial_guess, lower_bound, upper_bound]
dictPriors[&quot;alpha&quot;] = [0.5, 0.001, 1.0]
dictPriors[&quot;beta1&quot;] = [0.5, 0.001, 1.0]
dictPriors[&quot;beta2&quot;] = [0.5, 0.001, 1.0]
set_priors!(myProblem, dictPriors)</code></pre><h3 id="Step-4:-Specifying-the-function-generating-simulated-moments"><a class="docs-heading-anchor" href="#Step-4:-Specifying-the-function-generating-simulated-moments">Step 4: Specifying the function generating simulated moments</a><a id="Step-4:-Specifying-the-function-generating-simulated-moments-1"></a><a class="docs-heading-anchor-permalink" href="#Step-4:-Specifying-the-function-generating-simulated-moments" title="Permalink"></a></h3><p>The objective function must generate an <strong>ordered dictionary</strong> containing the <strong>keys of dictEmpiricalMoments</strong>. Use <code>set_simulate_empirical_moments!</code> and <code>construct_objective_function!</code></p><p><strong>Remark:</strong> we &quot;freeze&quot; randomness during the minimization step. One way to do that is to generate draws from a Uniform([0,1]) outside of the objective function and to use <a href="https://en.wikipedia.org/wiki/Inverse_transform_sampling">inverse transform sampling</a> to generate draws from a normal distribution. Otherwise the objective function would be &quot;noisy&quot; and the minimization algorithms would have a hard time finding the global minimum.</p><pre><code class="language-"># x[1] corresponds to the intercept; x[2] corresponds to beta1; x[3] corresponds to beta2
function functionLinearModel(x; uniform_draws::Array{Float64,1}, simX::Array{Float64,2}, nbDraws::Int64 = length(uniform_draws), burnInPerc::Int64 = 10)
    T = nbDraws
    P = 2       #number of dependent variables
    alpha = x[1]
    beta = x[2:end]
    theta = 0.0     #coefficient to create serial correlation in the error terms

    # Creation of error terms
    # row = individual dimension
    # column = time dimension
    U = zeros(T)
    d = Normal()
    # Inverse cdf (i.e. quantile)
    gaussian_draws = quantile.(d, uniform_draws)
    U[1] = gaussian_draws[1] #first error term
    for t = 2:T
        U[t] = gaussian_draws[t] + theta*U[t-1]
    end

    # Let&#39;s calculate the resulting y_t
    y = zeros(T)
    for t=1:T
        y[t] = alpha + simX[t,1]*beta[1] + simX[t,2]*beta[2] + U[t]
    end

    # Get rid of the burn-in phase:
    #------------------------------
    startT = div(nbDraws, burnInPerc)

    # Moments:
    #---------
    output = OrderedDict{String,Float64}()
    output[&quot;mean&quot;] = mean(y[startT:nbDraws])
    output[&quot;mean_x1y&quot;] = mean(simX[startT:nbDraws,1] .* y[startT:nbDraws])
    output[&quot;mean_x2y&quot;] = mean(simX[startT:nbDraws,2] .* y[startT:nbDraws])
    output[&quot;mean_x1y^2&quot;] = mean((simX[startT:nbDraws,1] .* y[startT:nbDraws]).^2)
    output[&quot;mean_x2y^2&quot;] = mean((simX[startT:nbDraws,2] .* y[startT:nbDraws]).^2)

    return output
end

# Let&#39;s freeze the randomness during the minimization
d_Uni = Uniform(0,1)
nbDraws = 100000 #number of draws in the simulated data
uniform_draws = rand(d_Uni, nbDraws)
simX = zeros(length(uniform_draws), 2)
d = Uniform(0, 5)
for p = 1:2
  simX[:,p] = rand(d, length(uniform_draws))
end

# Attach the function parameters -&gt; simulated moments:
set_simulate_empirical_moments!(myProblem, x -&gt; functionLinearModel(x, uniform_draws = uniform_draws, simX = simX))

# Construct the objective (m-m*)&#39;W(m-m*):
construct_objective_function!(myProblem)</code></pre><h3 id="Step-5.-Running-the-optimization"><a class="docs-heading-anchor" href="#Step-5.-Running-the-optimization">Step 5. Running the optimization</a><a id="Step-5.-Running-the-optimization-1"></a><a class="docs-heading-anchor-permalink" href="#Step-5.-Running-the-optimization" title="Permalink"></a></h3><p>Use the global optimization algorithm specified in <code>globalOptimizer</code>:</p><pre><code class="language-"># Global optimization:
msm_optimize!(myProblem, verbose = false)</code></pre><h3 id="Step-6.-Analysing-Results"><a class="docs-heading-anchor" href="#Step-6.-Analysing-Results">Step 6. Analysing Results</a><a id="Step-6.-Analysing-Results-1"></a><a class="docs-heading-anchor-permalink" href="#Step-6.-Analysing-Results" title="Permalink"></a></h3><h4 id="Step-6.A.-Point-estimates"><a class="docs-heading-anchor" href="#Step-6.A.-Point-estimates">Step 6.A. Point estimates</a><a id="Step-6.A.-Point-estimates-1"></a><a class="docs-heading-anchor-permalink" href="#Step-6.A.-Point-estimates" title="Permalink"></a></h4><pre><code class="language-">minimizer = msm_minimizer(myProblem)
minimum_val = msm_minimum(myProblem)
println(&quot;Minimum objective function = $(minimum_val)&quot;)
println(&quot;Estimated value for alpha = $(minimizer[1]). True value for beta1 = $(alpha0[1])&quot;)
println(&quot;Estimated value for beta1 = $(minimizer[2]). True value for beta1 = $(beta0[1])&quot;)
println(&quot;Estimated value for beta2 = $(minimizer[3]). True value for beta2 = $(beta0[2])&quot;)</code></pre><pre><code class="language-julia">Minimum objective function = 3.364713342376503e-6
Estimated value for alpha = 0.5725856664135125. True value for beta1 = 0.5662374165061859
Estimated value for beta1 = 0.5832878335694766. True value for beta1 = 0.5908446386657102
Estimated value for beta2 = 0.7664889629032697. True value for beta2 = 0.7667970365022592</code></pre><h4 id="Step-6.B.-Inference"><a class="docs-heading-anchor" href="#Step-6.B.-Inference">Step 6.B. Inference</a><a id="Step-6.B.-Inference-1"></a><a class="docs-heading-anchor-permalink" href="#Step-6.B.-Inference" title="Permalink"></a></h4><h5 id="Estimation-of-the-distance-matrix-\\Sigma_0"><a class="docs-heading-anchor" href="#Estimation-of-the-distance-matrix-\\Sigma_0">Estimation of the distance matrix <span>$\Sigma_0$</span></a><a id="Estimation-of-the-distance-matrix-\\Sigma_0-1"></a><a class="docs-heading-anchor-permalink" href="#Estimation-of-the-distance-matrix-\\Sigma_0" title="Permalink"></a></h5><p>Let&#39;s calculate the variance-covariance matrix of the <strong>&quot;distance matrix&quot;</strong> (using the terminolgy of <a href="https://www.jstor.org/stable/2951768?seq=1">Duffie and Singleton (1993)</a>). Here we know that errors are not correlated (the serial correlation coefficient is set to 0 in the code above). in the presence of serial correlation, an HAC estimation would be needed.</p><pre><code class="language-"># Empirical Series
#-----------------
X = zeros(T, 5)
X[:,1] = y
X[:,2] = (x[:,1] .* y)
X[:,3] = (x[:,2] .* y)
X[:,4] = (x[:,1] .* y).^2
X[:,5] = (x[:,2] .* y).^2
Sigma0 = cov(X)</code></pre><h5 id="Asymptotic-variance"><a class="docs-heading-anchor" href="#Asymptotic-variance">Asymptotic variance</a><a id="Asymptotic-variance-1"></a><a class="docs-heading-anchor-permalink" href="#Asymptotic-variance" title="Permalink"></a></h5><h6 id="Theory"><a class="docs-heading-anchor" href="#Theory">Theory</a><a id="Theory-1"></a><a class="docs-heading-anchor-permalink" href="#Theory" title="Permalink"></a></h6><p>The asymptotic variance of the MSM estimate is calculated using the usual <strong>GMM sandwich formula</strong>, corrected to take into account simulation noise.</p><p class="math-container">\[AsymptoticVarianceMSM = (1 + \tau)*AsymptoticVarianceGMM\]</p><p>Here we are trying to match unconditional moments from time series. In this case, <span>$\tau = \frac{tData}{tSimulation}$</span>, where <span>$tData$</span> is the number of periods in the empirical data and <span>$tSimulation$</span> is the number of time periods in the simulated data.</p><p>See <a href="https://www.jstor.org/stable/2951768?seq=1">Duffie and Singleton (1993)</a> and <a href="https://www.jstor.org/stable/3533164?seq=1">Gouriéroux and Montfort (1996)</a> for details on how to choose <span>$\tau$</span>.</p><h6 id="Practice"><a class="docs-heading-anchor" href="#Practice">Practice</a><a id="Practice-1"></a><a class="docs-heading-anchor-permalink" href="#Practice" title="Permalink"></a></h6><p>Calculating the asymptotic variance using MSM.jl is done in two steps:</p><ul><li>setting the value of the <strong>&quot;distance matrix&quot;</strong> using the function <code>set_Sigma0!</code></li><li>calculating the asymptotic variance using the function <code>calculate_Avar!</code></li></ul><pre><code class="language-">set_Sigma0!(myProblem, Sigma0)
calculate_Avar!(myProblem, minimizer, tau = T/nbDraws) # nbDraws = number of draws in the simulated data</code></pre><h4 id="Step-6.C.-Summarizing-the-results"><a class="docs-heading-anchor" href="#Step-6.C.-Summarizing-the-results">Step 6.C. Summarizing the results</a><a id="Step-6.C.-Summarizing-the-results-1"></a><a class="docs-heading-anchor-permalink" href="#Step-6.C.-Summarizing-the-results" title="Permalink"></a></h4><p>Once the asymptotic variance has been calculated, a summary table can be obtained using the function <code>summary_table</code>. This function has four inputs:</p><ol><li>a MSMProblem</li><li>the minimizer of the objective function</li><li>the length of the empirical sample</li><li>the confidence level associated to the test <strong>H0:</strong> <span>$\theta_i = 0$</span>,  <strong>H1:</strong> <span>$\theta_i != 0$</span></li></ol><pre><code class="language-">df = summary_table(myProblem, minimizer, T, 0.05)</code></pre><table><tr><th style="text-align: right">Estimate</th><th style="text-align: right">StdError</th><th style="text-align: right">tValue</th><th style="text-align: right">pValue</th><th style="text-align: right">ConfIntervalLower</th><th style="text-align: right">ConfIntervalUpper</th></tr><tr><td style="text-align: right">Float64</td><td style="text-align: right">Float64</td><td style="text-align: right">Float64</td><td style="text-align: right">Float64</td><td style="text-align: right">Float64</td><td style="text-align: right">Float64</td></tr><tr><td style="text-align: right">0.572586</td><td style="text-align: right">0.0228065</td><td style="text-align: right">25.1062</td><td style="text-align: right">0.0</td><td style="text-align: right">0.535072</td><td style="text-align: right">0.610099</td></tr><tr><td style="text-align: right">0.583288</td><td style="text-align: right">0.00868862</td><td style="text-align: right">67.1324</td><td style="text-align: right">0.0</td><td style="text-align: right">0.568996</td><td style="text-align: right">0.597579</td></tr><tr><td style="text-align: right">0.766489</td><td style="text-align: right">0.0081483</td><td style="text-align: right">94.0674</td><td style="text-align: right">0.0</td><td style="text-align: right">0.753086</td><td style="text-align: right">0.779892</td></tr></table><h2 id="Example-in-parallel"><a class="docs-heading-anchor" href="#Example-in-parallel">Example in parallel</a><a id="Example-in-parallel-1"></a><a class="docs-heading-anchor-permalink" href="#Example-in-parallel" title="Permalink"></a></h2><p>To use the package on a cluster, one must make sure that empirical moments, priors and the weight matrix are defined for each worker. This can be done using <code>@everywhere begin end</code> blocks, or by using <a href="https://github.com/ChrisRackauckas/ParallelDataTransfer.jl">ParallelDataTransfer.jl</a>. The function returning simulated moments must also be defined <code>@everywhere</code>. See the file <a href="https://github.com/JulienPascal/MSM.jl/blob/main/notebooks/LinearModelCluster.jl">LinearModelCluster.jl</a> for details.</p><h3 id="Option-1:-Global-parallel-optimization"><a class="docs-heading-anchor" href="#Option-1:-Global-parallel-optimization">Option 1: Global parallel optimization</a><a id="Option-1:-Global-parallel-optimization-1"></a><a class="docs-heading-anchor-permalink" href="#Option-1:-Global-parallel-optimization" title="Permalink"></a></h3><p>Choose a global optimizer that <strong>supports parallel evaluations</strong> (e.g. xnes or dxnes). See the <a href="https://github.com/robertfeldt/BlackBoxOptim.jl">documentation</a> for BlackBoxOptim.jl.</p><pre><code class="language-">msm_optimize!(myProblem, verbose = false)

# Access the results using best_candidate
minimizer = msm_minimizer(myProblem)
minimum_val = msm_minimum(myProblem)</code></pre><h3 id="Option-2:-Multistart-algorithm"><a class="docs-heading-anchor" href="#Option-2:-Multistart-algorithm">Option 2: Multistart algorithm</a><a id="Option-2:-Multistart-algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#Option-2:-Multistart-algorithm" title="Permalink"></a></h3><p>The function <code>msm_multistart!</code> searches for starting values for which the model converges. Then, several local optimization algorithms (specified with <code>localOptimizer</code>) are started in parallel. The &quot;global&quot; minimum is the minimum of the local minima:</p><pre><code class="language-">msm_multistart!(myProblem, nums = nworkers(), verbose = false)
minimizer_multistart = msm_multistart_minimizer(myProblem)
minimum_multistart = msm_multistart_minimum(myProblem)</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../installation/">« Installation</a><a class="docs-footer-nextpage" href="../functions/">Functions and Types »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Friday 28 May 2021 11:50">Friday 28 May 2021</span>. Using Julia version 1.6.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
