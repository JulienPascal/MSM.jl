var documenterSearchIndex = {"docs":
[{"location":"gettingstarted/#Getting-Started","page":"Getting started","title":"Getting Started","text":"","category":"section"},{"location":"gettingstarted/","page":"Getting started","title":"Getting started","text":"Our overarching goal is to find the parameter values theta_MSM minimizing the following function:","category":"page"},{"location":"gettingstarted/","page":"Getting started","title":"Getting started","text":"g(theta m* W) = (m(theta) - m*) W (m(theta) - m*)","category":"page"},{"location":"gettingstarted/","page":"Getting started","title":"Getting started","text":"where m* is a vector of empirical moments, m(theta) is a vector of moments calculated using simulated data, and W is carefully chosen weighting matrix. We also want to build confidence intervals for theta_MSM.","category":"page"},{"location":"gettingstarted/","page":"Getting started","title":"Getting started","text":"While simple in theory (it is just a function minimization, right?), in practice many bad things can happen. The function g may fail in some areas of the parameter space; g may be stuck in some local minima; g is really slow and you do not have a strong prior regarding good starting values. MSM.jl uses minimization algorithms that are robust to the problems mentioned above. You may choose between two options:","category":"page"},{"location":"gettingstarted/","page":"Getting started","title":"Getting started","text":"Global minimization algorithms from BlackBoxOptim\nA multistart algorithm using several local optimization routines from Optim.jl","category":"page"},{"location":"gettingstarted/","page":"Getting started","title":"Getting started","text":"Let's follow a learning-by-doing approach. As a warm-up, let's first estimate parameters in serial. In a second step, we use several workers on a cluster.","category":"page"},{"location":"gettingstarted/#Example-in-serial","page":"Getting started","title":"Example in serial","text":"","category":"section"},{"location":"gettingstarted/","page":"Getting started","title":"Getting started","text":"In a real-world scenario, you would probably use empirical data. Here, let's simulate a fake dataset.","category":"page"},{"location":"gettingstarted/","page":"Getting started","title":"Getting started","text":"using MSM\nusing DataStructures\nusing OrderedCollections\nusing Random\nusing Distributions\nusing Statistics\nusing LinearAlgebra\nusing Distributed\nusing Plots\nhh = 750; nothing # hide\nww = round(Int, (16/9)*hh); nothing # hide\ngr(size = (ww,hh)); nothing # hide\nRandom.seed!(1234)  #for replicability reasons\nT = 100000          #number of periods\nP = 2               #number of dependent variables\nbeta0 = rand(P)     #choose true coefficients by drawing from a uniform distribution on [0,1]\nalpha0 = rand(1)[]  #intercept\ntheta0 = 0.0 #coefficient to create serial correlation in the error terms\n\n# Generation of error terms\n# row = individual dimension\n# column = time dimension\nU = zeros(T)\nd = Normal()\nU[1] = rand(d, 1)[] #first error term\nfor t = 2:T\n    U[t] = rand(d, 1)[] + theta0*U[t-1]\nend\n\n# Let's simulate the dependent variables x_t\nx = zeros(T, P)\nd = Uniform(0, 5)\nfor p = 1:P  \n    x[:,p] = rand(d, T)\nend\n\n# Let's calculate the resulting y_t\ny = zeros(T)\nfor t=1:T\n    y[t] = alpha0 + x[t,1]*beta0[1] + x[t,2]*beta0[2] + U[t]\nend\n\n# Visualize data\np1 = scatter(x[1:100,1], y[1:100], xlabel = \"x1\", ylabel = \"y\", legend=:none, smooth=true)\np2 = scatter(x[1:100,2], y[1:100], xlabel = \"x2\", ylabel = \"y\", legend=:none, smooth=true)\np = plot(p1, p2);\nplot!(p, size = (ww,hh)); nothing # hide\nsavefig(p, \"f-fake-data.png\"); nothing # hide","category":"page"},{"location":"gettingstarted/","page":"Getting started","title":"Getting started","text":"(Image: )","category":"page"},{"location":"gettingstarted/#Step-1:-Initializing-a-MSMProblem","page":"Getting started","title":"Step 1: Initializing a MSMProblem","text":"","category":"section"},{"location":"gettingstarted/","page":"Getting started","title":"Getting started","text":"# Select a global optimizer (see BlackBoxOptim.jl) and a local minimizer (see Optim.jl):\nmyProblem = MSMProblem(options = MSMOptions(maxFuncEvals=10000, globalOptimizer = :dxnes, localOptimizer = :LBFGS));","category":"page"},{"location":"gettingstarted/#Step-2.-Set-empirical-moments-and-weight-matrix","page":"Getting started","title":"Step 2. Set empirical moments and weight matrix","text":"","category":"section"},{"location":"gettingstarted/","page":"Getting started","title":"Getting started","text":"Choose the set of empirical moments to match and the weight matrix W using the functions set_empirical_moments! and set_weight_matrix!","category":"page"},{"location":"gettingstarted/","page":"Getting started","title":"Getting started","text":"dictEmpiricalMoments = OrderedDict{String,Array{Float64,1}}()\ndictEmpiricalMoments[\"mean\"] = [mean(y)] #informative on the intercept\ndictEmpiricalMoments[\"mean_x1y\"] = [mean(x[:,1] .* y)] #informative on betas\ndictEmpiricalMoments[\"mean_x2y\"] = [mean(x[:,2] .* y)] #informative on betas\ndictEmpiricalMoments[\"mean_x1y^2\"] = [mean((x[:,1] .* y).^2)] #informative on betas\ndictEmpiricalMoments[\"mean_x2y^2\"] = [mean((x[:,2] .* y).^2)] #informative on betas\n\nW = Matrix(1.0 .* I(length(dictEmpiricalMoments)))#initialization\n#Special case: diagonal matrix\n#Sum of square percentage deviations from empirical moments\n#(you may choose something else)\nfor (indexMoment, k) in enumerate(keys(dictEmpiricalMoments))\n    W[indexMoment,indexMoment] = 1.0/(dictEmpiricalMoments[k][1])^2\nend\n\nset_empirical_moments!(myProblem, dictEmpiricalMoments)\nset_weight_matrix!(myProblem, W)","category":"page"},{"location":"gettingstarted/#Step-3.-Set-priors","page":"Getting started","title":"Step 3. Set priors","text":"","category":"section"},{"location":"gettingstarted/","page":"Getting started","title":"Getting started","text":"Our \"prior\" belief regarding the parameter values is to be specified using set_priors!(). It is not fully a full-fledged prior probability distribution, but simply an initial guess for each parameter, as well as lower and upper bounds:","category":"page"},{"location":"gettingstarted/","page":"Getting started","title":"Getting started","text":"dictPriors = OrderedDict{String,Array{Float64,1}}()\n# Of the form: [initial_guess, lower_bound, upper_bound]\ndictPriors[\"alpha\"] = [0.5, 0.001, 1.0]\ndictPriors[\"beta1\"] = [0.5, 0.001, 1.0]\ndictPriors[\"beta2\"] = [0.5, 0.001, 1.0]\nset_priors!(myProblem, dictPriors)","category":"page"},{"location":"gettingstarted/#Step-4:-Specifying-the-function-generating-simulated-moments","page":"Getting started","title":"Step 4: Specifying the function generating simulated moments","text":"","category":"section"},{"location":"gettingstarted/","page":"Getting started","title":"Getting started","text":"The objective function must generate an ordered dictionary containing the keys of dictEmpiricalMoments. Use set_simulate_empirical_moments! and construct_objective_function!","category":"page"},{"location":"gettingstarted/","page":"Getting started","title":"Getting started","text":"Remark: we \"freeze\" randomness during the minimization step. One way to do that is to generate draws from a Uniform([0,1]) outside of the objective function and to use inverse transform sampling to generate draws from a normal distribution. Otherwise the objective function would be \"noisy\" and the minimization algorithms would have a hard time finding the global minimum.","category":"page"},{"location":"gettingstarted/","page":"Getting started","title":"Getting started","text":"# x[1] corresponds to the intercept; x[2] corresponds to beta1; x[3] corresponds to beta2\nfunction functionLinearModel(x; uniform_draws::Array{Float64,1}, simX::Array{Float64,2}, nbDraws::Int64 = length(uniform_draws), burnInPerc::Int64 = 0)\n    T = nbDraws\n    P = 2       #number of dependent variables\n    alpha = x[1]\n    beta = x[2:end]\n    theta = 0.0     #coefficient to create serial correlation in the error terms\n\n    # Creation of error terms\n    # row = individual dimension\n    # column = time dimension\n    U = zeros(T)\n    d = Normal()\n    # Inverse cdf (i.e. quantile)\n    gaussian_draws = quantile.(d, uniform_draws)\n    U[1] = gaussian_draws[1] #first error term\n    for t = 2:T\n        U[t] = gaussian_draws[t] + theta*U[t-1]\n    end\n\n    # Let's calculate the resulting y_t\n    y = zeros(T)\n    for t=1:T\n        y[t] = alpha + simX[t,1]*beta[1] + simX[t,2]*beta[2] + U[t]\n    end\n\n    # Get rid of the burn-in phase:\n    #------------------------------\n    startT = max(1, Int(nbDraws * (burnInPerc / 100)))\n\n    # Moments:\n    #---------\n    output = OrderedDict{String,Float64}()\n    output[\"mean\"] = mean(y[startT:nbDraws])\n    output[\"mean_x1y\"] = mean(simX[startT:nbDraws,1] .* y[startT:nbDraws])\n    output[\"mean_x2y\"] = mean(simX[startT:nbDraws,2] .* y[startT:nbDraws])\n    output[\"mean_x1y^2\"] = mean((simX[startT:nbDraws,1] .* y[startT:nbDraws]).^2)\n    output[\"mean_x2y^2\"] = mean((simX[startT:nbDraws,2] .* y[startT:nbDraws]).^2)\n\n    return output\nend\n\n# Let's freeze the randomness during the minimization\nd_Uni = Uniform(0,1)\nnbDraws = 100000 #number of draws in the simulated data\nuniform_draws = rand(d_Uni, nbDraws)\nsimX = zeros(length(uniform_draws), 2)\nd = Uniform(0, 5)\nfor p = 1:2\n  simX[:,p] = rand(d, length(uniform_draws))\nend\n\n# Attach the function parameters -> simulated moments:\nset_simulate_empirical_moments!(myProblem, x -> functionLinearModel(x, uniform_draws = uniform_draws, simX = simX))\n\n# Construct the objective (m-m*)'W(m-m*):\nconstruct_objective_function!(myProblem)","category":"page"},{"location":"gettingstarted/#Step-5.-Running-the-optimization","page":"Getting started","title":"Step 5. Running the optimization","text":"","category":"section"},{"location":"gettingstarted/","page":"Getting started","title":"Getting started","text":"Use the global optimization algorithm specified in globalOptimizer:","category":"page"},{"location":"gettingstarted/","page":"Getting started","title":"Getting started","text":"# Global optimization:\nmsm_optimize!(myProblem, verbose = false)","category":"page"},{"location":"gettingstarted/#Step-6.-Analysing-Results","page":"Getting started","title":"Step 6. Analysing Results","text":"","category":"section"},{"location":"gettingstarted/#Step-6.A.-Point-estimates","page":"Getting started","title":"Step 6.A. Point estimates","text":"","category":"section"},{"location":"gettingstarted/","page":"Getting started","title":"Getting started","text":"minimizer = msm_minimizer(myProblem)\nminimum_val = msm_minimum(myProblem)\nprintln(\"Minimum objective function = $(minimum_val)\")\nprintln(\"Estimated value for alpha = $(minimizer[1]). True value for beta1 = $(alpha0[1])\")\nprintln(\"Estimated value for beta1 = $(minimizer[2]). True value for beta1 = $(beta0[1])\")\nprintln(\"Estimated value for beta2 = $(minimizer[3]). True value for beta2 = $(beta0[2])\")","category":"page"},{"location":"gettingstarted/#Step-6.B.-Inference","page":"Getting started","title":"Step 6.B. Inference","text":"","category":"section"},{"location":"gettingstarted/#Estimation-of-the-distance-matrix-\\Sigma_0","page":"Getting started","title":"Estimation of the distance matrix Sigma_0","text":"","category":"section"},{"location":"gettingstarted/","page":"Getting started","title":"Getting started","text":"Let's calculate the variance-covariance matrix of the \"distance matrix\" (using the terminolgy of Duffie and Singleton (1993)). Here we know that errors are not correlated (the serial correlation coefficient is set to 0 in the code above). in the presence of serial correlation, an HAC estimation would be needed.","category":"page"},{"location":"gettingstarted/","page":"Getting started","title":"Getting started","text":"# Empirical Series\n#-----------------\nX = zeros(T, 5)\nX[:,1] = y\nX[:,2] = (x[:,1] .* y)\nX[:,3] = (x[:,2] .* y)\nX[:,4] = (x[:,1] .* y).^2\nX[:,5] = (x[:,2] .* y).^2\nSigma0 = cov(X)","category":"page"},{"location":"gettingstarted/#Asymptotic-variance","page":"Getting started","title":"Asymptotic variance","text":"","category":"section"},{"location":"gettingstarted/#Theory","page":"Getting started","title":"Theory","text":"","category":"section"},{"location":"gettingstarted/","page":"Getting started","title":"Getting started","text":"The asymptotic variance of the MSM estimate is calculated using the usual GMM sandwich formula, corrected to take into account simulation noise.","category":"page"},{"location":"gettingstarted/","page":"Getting started","title":"Getting started","text":"AsymptoticVarianceMSM = (1 + tau)*AsymptoticVarianceGMM","category":"page"},{"location":"gettingstarted/","page":"Getting started","title":"Getting started","text":"Here we are trying to match unconditional moments from time series. In this case, tau = fractDatatSimulation, where tData is the number of periods in the empirical data and tSimulation is the number of time periods in the simulated data.","category":"page"},{"location":"gettingstarted/","page":"Getting started","title":"Getting started","text":"See Duffie and Singleton (1993) and Gouriéroux and Montfort (1996) for details on how to choose tau.","category":"page"},{"location":"gettingstarted/#Practice","page":"Getting started","title":"Practice","text":"","category":"section"},{"location":"gettingstarted/","page":"Getting started","title":"Getting started","text":"Calculating the asymptotic variance using MSM.jl is done in two steps:","category":"page"},{"location":"gettingstarted/","page":"Getting started","title":"Getting started","text":"setting the value of the \"distance matrix\" using the function set_Sigma0!\ncalculating the asymptotic variance using the function calculate_Avar!","category":"page"},{"location":"gettingstarted/","page":"Getting started","title":"Getting started","text":"set_Sigma0!(myProblem, Sigma0)\ncalculate_Avar!(myProblem, minimizer, tau = T/nbDraws) # nbDraws = number of draws in the simulated data","category":"page"},{"location":"gettingstarted/#Step-6.C.-Summarizing-the-results","page":"Getting started","title":"Step 6.C. Summarizing the results","text":"","category":"section"},{"location":"gettingstarted/","page":"Getting started","title":"Getting started","text":"Once the asymptotic variance has been calculated, a summary table can be obtained using the function summary_table. This function has four inputs:","category":"page"},{"location":"gettingstarted/","page":"Getting started","title":"Getting started","text":"a MSMProblem\nthe minimizer of the objective function\nthe length of the empirical sample\nthe confidence level associated to the test H0: theta_i = 0,  H1: theta_i = 0","category":"page"},{"location":"gettingstarted/","page":"Getting started","title":"Getting started","text":"df = summary_table(myProblem, minimizer, T, 0.05)","category":"page"},{"location":"gettingstarted/#Step-7.-Identification-checks","page":"Getting started","title":"Step 7. Identification checks","text":"","category":"section"},{"location":"gettingstarted/","page":"Getting started","title":"Getting started","text":"Local identification requires that the Jacobian matrix of the function f(theta) - m(theta) to be full column rank in a neighborhood of the solution:","category":"page"},{"location":"gettingstarted/","page":"Getting started","title":"Getting started","text":"D = calculate_D(myProblem, minimizer)\nprintln(\"number of parameters: $(size(D,2))\")\nprintln(\"rank of D is: $(rank(D))\")","category":"page"},{"location":"gettingstarted/","page":"Getting started","title":"Getting started","text":"Local identification can also be visually checked by inspecting slices of the objective function in a neighborhood of the estimated value:","category":"page"},{"location":"gettingstarted/","page":"Getting started","title":"Getting started","text":"vXGrid, vYGrid = msm_slices(myProblem, minimizer, nbPoints = 7);\n\nusing LaTeXStrings;\np1 = plot(vXGrid[:, 1],vYGrid[:, 1],title = L\"\\alpha\", label = \"\",linewidth = 3, xrotation = 45);\nplot!(p1, [minimizer[1]], seriestype = :vline, label = \"\",linewidth = 1);\np2 = plot(vXGrid[:, 2],vYGrid[:, 2],title = L\"\\beta_1\", label = \"\",linewidth = 3, xrotation = 45);\nplot!(p2, [minimizer[2]], seriestype = :vline, label = \"\",linewidth = 1);\np3 = plot(vXGrid[:, 3],vYGrid[:, 3],title = L\"\\beta_2\", label = \"\",linewidth = 3, xrotation = 45);\nplot!(p3, [minimizer[3]], seriestype = :vline, label = \"\",linewidth = 1);\nplot_combined = plot(p1, p2, p3);\nplot!(plot_combined, size = (ww,hh)); nothing # hide\nsavefig(plot_combined, \"slices.png\"); nothing # hide","category":"page"},{"location":"gettingstarted/","page":"Getting started","title":"Getting started","text":"(Image: )","category":"page"},{"location":"gettingstarted/#Example-in-parallel","page":"Getting started","title":"Example in parallel","text":"","category":"section"},{"location":"gettingstarted/","page":"Getting started","title":"Getting started","text":"To use the package on a cluster, one must make sure that empirical moments, priors and the weight matrix are defined for each worker. This can be done using @everywhere begin end blocks, or by using ParallelDataTransfer.jl. The function returning simulated moments must also be defined @everywhere. See the file LinearModelCluster.jl for details.","category":"page"},{"location":"gettingstarted/#Option-1:-Global-parallel-optimization","page":"Getting started","title":"Option 1: Global parallel optimization","text":"","category":"section"},{"location":"gettingstarted/","page":"Getting started","title":"Getting started","text":"Choose a global optimizer that supports parallel evaluations (e.g. xnes or dxnes). See the documentation for BlackBoxOptim.jl.","category":"page"},{"location":"gettingstarted/","page":"Getting started","title":"Getting started","text":"msm_optimize!(myProblem, verbose = false)\nminimizer = msm_minimizer(myProblem)\nminimum_val = msm_minimum(myProblem)","category":"page"},{"location":"gettingstarted/#Option-2:-Multistart-algorithm","page":"Getting started","title":"Option 2: Multistart algorithm","text":"","category":"section"},{"location":"gettingstarted/","page":"Getting started","title":"Getting started","text":"The function msm_multistart! proceeds in two steps:","category":"page"},{"location":"gettingstarted/","page":"Getting started","title":"Getting started","text":"It searches for starting values for which the model converges.\nSeveral local optimization algorithms (specified with localOptimizer) are started in parallel using promising starting values from step 1","category":"page"},{"location":"gettingstarted/","page":"Getting started","title":"Getting started","text":"The \"global\" minimum is the minimum of the local minima:","category":"page"},{"location":"gettingstarted/","page":"Getting started","title":"Getting started","text":"msm_multistart!(myProblem, nums = nworkers(), verbose = false)\nminimizer_multistart = msm_multistart_minimizer(myProblem)\nminimum_multistart = msm_multistart_minimum(myProblem)","category":"page"},{"location":"functions/#Functions-and-Types","page":"Functions and Types","title":"Functions and Types","text":"","category":"section"},{"location":"functions/","page":"Functions and Types","title":"Functions and Types","text":"MSMOptions","category":"page"},{"location":"functions/#MSM.MSMOptions","page":"Functions and Types","title":"MSM.MSMOptions","text":"MSMOptions\n\nMSMOptions is a mutable struct that contains options related to the optimization.\n\nExamples\n\njulia> options = MSMOptions(maxFuncEvals=1000, globalOptimizer = :dxnes, localOptimizer = :LBFGS)\n\n\n\n\n\n","category":"type"},{"location":"functions/","page":"Functions and Types","title":"Functions and Types","text":"For example, let's say we want to estimate a model with a maximum of 1000 function evaluations (only the global optimizer will take into consideration this constraint). We want to use :dxnes as our global optimizer algorithm and :LBFGS for our local minimization algorithm. Global optimizers can be chosen from BlackBoxOptim.jl, while local optimizer are to be chosen from Optim.jl:","category":"page"},{"location":"functions/","page":"Functions and Types","title":"Functions and Types","text":"options = MSMOptions(maxFuncEvals=1000, globalOptimizer = :dxnes, localOptimizer = :LBFGS);","category":"page"},{"location":"references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"Duffie, Darrell, and Kenneth J. Singleton. \"Simulated Moments Estimation of Markov Models of Asset Prices.\" Econometrica 61.4 (1993): 929-952.\nGourieroux, Monfort, et al. Simulation-based econometric methods. Oxford university press, 1996.\nLee, Bong-Soo, and Beth Fisher Ingram. \"Simulation estimation of time-series models.\" Journal of Econometrics 47.2-3 (1991): 197-205.\nMcFadden, Daniel. \"A method of simulated moments for estimation of discrete response models without numerical integration.\" Econometrica: Journal of the Econometric Society (1989): 995-1026.\nRuge-Murcia, Francisco. \"Estimating nonlinear DSGE models by the simulated method of moments: With an application to business cycles.\" Journal of Economic Dynamics and Control 36.6 (2012): 914-938.","category":"page"},{"location":"license/#Licence","page":"License","title":"Licence","text":"","category":"section"},{"location":"license/","page":"License","title":"License","text":"See LICENSE.md","category":"page"},{"location":"contributing/#Contributing","page":"Contributing","title":"Contributing","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Contributions welcome!","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"See CONTRIBUTING.md","category":"page"},{"location":"installation/#Installation","page":"Installation","title":"Installation","text":"","category":"section"},{"location":"installation/","page":"Installation","title":"Installation","text":"You can install MSM.jl in two steps:","category":"page"},{"location":"installation/#Step-1","page":"Installation","title":"Step 1","text":"","category":"section"},{"location":"installation/","page":"Installation","title":"Installation","text":"Enter the Pkg REPL by pressing ] from the Julia REPL (to get back to the Julia REPL, press backspace or ^C. see Pkg.jl)","category":"page"},{"location":"installation/#Step-2","page":"Installation","title":"Step 2","text":"","category":"section"},{"location":"installation/","page":"Installation","title":"Installation","text":"To add a package, use add:","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"pkg> add https://github.com/JulienPascal/MSM.jl.git","category":"page"},{"location":"#MSM.jl","page":"Home","title":"MSM.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation Build Status Coverage\n(Image: ) (Image: Build Status) (Image: codecov.io)\n (Image: Build Status) ","category":"page"},{"location":"","page":"Home","title":"Home","text":"MSM.jl is a package designed to facilitate the estimation of economic models via the Method of Simulated Moments.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#Why","page":"Home","title":"Why","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"An economic theory can be written as a system of equations that depends on primitive parameters. The aim of the econometrician is to recover the unknown parameters using empirical data. One popular approach is to maximize the likelihood funtion. Yet in many instances, the likelihood function is intractable. An alternative approach to estimate the unknown parameters is to minimize a (weighted) distance between the empirical moments and their theoretical counterparts.","category":"page"},{"location":"","page":"Home","title":"Home","text":"When the function mapping the set of parameter values to the theoretical moments (the expected response function) is known, this method is called the Generalized Method of Moments. However, in many interesting cases the expected response function is unknown. This issue may be circumvented by simulating the expected response function, which is often an easy task. In this case, the method is called the Method of Simulated Moments.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#Philosophy","page":"Home","title":"Philosophy","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"MSM.jl is being developed with the following constraints in mind:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Parallelization within the expected response function is difficult to achieve. This is generally the case when working with the simulated method of moments, as the simulated time series are often serially correlated.\nThus, the minimizing algorithm should be able to run in parallel\nThe minimizing algorithm should search for a global minimum, as the objective function may have multiple local minima.\nDo not reinvent the wheel. Excellent minimization packages already exist in the Julia ecosystem. This is why MSM.jl relies on BlackBoxOptim.jl and Optim.jl to perform the minimization.","category":"page"}]
}
